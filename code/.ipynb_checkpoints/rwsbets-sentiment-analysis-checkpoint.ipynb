{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Wall Street Bets Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/reddit.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from matplotlib.pyplot import figure\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../reddit_wsb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gme = pd.read_csv('../gme.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns for analysis\n",
    "df = df.drop(columns=['id', 'url', 'created', 'Unnamed: 10', 'Dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gme = df_gme.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'Change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df.isnull().sum()\n",
    "df = df.dropna(subset=['timestamp', 'change (+/-)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gme.isnull().sum()\n",
    "df_gme = df_gme.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gme.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gme.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gme.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text): \n",
    "    # tokenization, remove punctuation, lemmatization\n",
    "    words = word_tokenize(str(text))\n",
    "    words = RegexpTokenizer(r'\\w+')\n",
    "    words = [porter.stem(t) for t in str(text).split()]\n",
    "    \n",
    "    # remove symbols, websites, email addresses \n",
    "    words = [re.sub(r\"[^A-Za-z@]\", \"\", word) for word in words] \n",
    "    words = [re.sub(r\"\\S+com\", \"\", word) for word in words]\n",
    "    words = [re.sub(r\"\\S+@\\S+\", \"\", word) for word in words] \n",
    "    words = [word for word in words if word != \" \" and word != \"\"]\n",
    "    words = [word for word in words if len(word) != 0] \n",
    "    \n",
    "    # remove stopwords     \n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(nltk.corpus.stopwords.words('english'))\n",
    "    stopwords_lower = [s.lower() for s in stopwords]\n",
    "    words=[word.lower() for word in words if word.lower() not in stopwords_lower]\n",
    "    \n",
    "    # combine a list into one string   \n",
    "    string = \" \".join(words)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_cleaned'] = df['title'].apply(text_preprocessing)\n",
    "df['body_cleaned'] = df['body'].apply(text_preprocessing)\n",
    "df['combined_cleaned'] = df['title_cleaned'] + ' ' + df['body_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles and bodies of the wsbets posts\n",
    "vis_df = df[['title', 'body', 'timestamp']].copy()\n",
    "vis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into plain text column\n",
    "vis_df = vis_df.dropna()\n",
    "vis_df['combined'] = vis_df['title'] + ' ' + vis_df['body']\n",
    "vis_df = vis_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run the same analysis for all posts\n",
    "text = ' '.join(vis_df.combined)\n",
    "print (\"There are {} words in the combination of all posts and titles on r/wsbets.\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword list\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GME stock prices over time\n",
    "fig = px.line(df_gme, x='Date', y='Adj Close', title=\"GME Stock Prices over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GME Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get only GME posts\n",
    "def getGME(text):\n",
    "    return \"gme\" in text.lower() or 'game stop' in text.lower() or 'gamestop' in text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframe based on posts with GME in them\n",
    "gme_sent_df = df[['timestamp', 'combined_cleaned', 'change (+/-)']].copy()\n",
    "gme_sent_df['containsGME'] = df['combined_cleaned'].apply(getGME)\n",
    "gme_sent_df.drop(gme_sent_df[gme_sent_df['containsGME'] == False].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean GME posts dataset\n",
    "gme_sent_df = gme_sent_df.drop(columns=['containsGME'])\n",
    "gme_sent_df = gme_sent_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gme_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(gme_sent_df['combined_cleaned'], \n",
    "                                                    gme_sent_df['change (+/-)'], \n",
    "                                                    random_state=0)\n",
    "# Build a machine learning pipeline\n",
    "est = Pipeline([('vectorizer', TfidfVectorizer(lowercase=False)),\n",
    " ('classifier', LogisticRegression(solver='liblinear', max_iter=1000))])\n",
    "\n",
    "# GridSearchCV with a transformer and a estimator\n",
    "parameters = {'vectorizer__max_df': (0.8,0.9), \n",
    " 'vectorizer__min_df': [20,50,0.1],\n",
    " \"classifier__C\": np.logspace(-3,3,7), \n",
    " \"classifier__penalty\" :[\"l1\", \"l2\"]}\n",
    "gs = GridSearchCV(est, param_grid=parameters)\n",
    "\n",
    "# Fit the training data\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking GME Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to get subjectivity and polarity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subjectivity and polarity of GME posts\n",
    "gme_sent_df['Subjectivity'] = gme_sent_df['combined'].apply(getSubjectivity)\n",
    "gme_sent_df['Polarity'] = gme_sent_df['combined'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gme_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the sentiment for each post\n",
    "def getSIA(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment scores for each post\n",
    "compound = []\n",
    "neg = []\n",
    "pos = []\n",
    "sent = []\n",
    "sent_value = []\n",
    "for i in range(0, len(gme_sent_df['combined'])):\n",
    "    SIA = getSIA(gme_sent_df['combined'][i])\n",
    "    compound.append(SIA['compound'])\n",
    "    neg.append(SIA['neg'])\n",
    "    neu.append(SIA['neu'])\n",
    "    pos.append(SIA['pos'])\n",
    "    max_value = max(SIA['neg'], SIA['pos'])\n",
    "    sent_value.append(max_value)\n",
    "    if max_value == SIA['neg']:\n",
    "        sent.append('negative')\n",
    "    else:\n",
    "        sent.append('positive')\n",
    "# Store sentiments inside dataframe\n",
    "gme_sent_df['Compound'] = compound\n",
    "gme_sent_df['Positive'] = pos\n",
    "gme_sent_df['Negative'] = neg\n",
    "gme_sent_df['Sentiment'] = sent\n",
    "gme_sent_df['Sentiment Value'] = sent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gme_sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GME Positive Sentiment Over Time\n",
    "pos_sent_df = gme_sent_df[gme_sent_df['Sentiment'] == 'positive'].reset_index()\n",
    "fig = px.scatter(pos_sent_df, x='timestamp', y=['Sentiment Value'], title=\"GME Positive Sentiment over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GME Negative Sentiment Over Time\n",
    "neg_sent_df = gme_sent_df[gme_sent_df['Sentiment'] == 'negative'].reset_index()\n",
    "fig = px.scatter(neg_sent_df, x='timestamp', y=['Sentiment Value'], title=\"GME Negative Sentiment over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
